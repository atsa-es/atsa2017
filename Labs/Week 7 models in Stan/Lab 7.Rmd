---
title: "Lab 7 - FISH 507"
author: "Eric Ward"
date: "February 16, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 7

## Installing packages
For this lab, we'll use [Stan](http://mc-stan.org/documentation/) for estimation (the last several times this course has been taught, we used JAGS or WinBUGS). You'll need to download Stan below. Additionally, you'll need to download a package we've created for deploying these models called *STATS*. This is hosted on the Github repository for the class [https://github.com/eric-ward/safs-timeseries](https://github.com/eric-ward/safs-timeseries), and you can install with *devtools*. These examples are primarily drawn from the *Stan* manual and previous code from this class.

```{r load, warning=FALSE, message=FALSE, results='hide'}
library(rstan)
library(devtools)
#if you have not installed statss
#devtools::install_github("eric-ward/safs-timeseries/statss")
library(statss)
# for optimizing stan on your machine,
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

## Data

For data for this lab, we'll include a dataset on airquality in New York. We'll load the data and create a couple new variables for future use. For the majority of our models, we're going to treat 'Wind' as the response variable for our time series models. 

```{r}
data(airquality)
Wind = airquality$Wind # wind speed
Temp = airquality$Temp # air temperature
```

## Linear regression

We'll start with the simplest time series model possible: linear regression with only an intercept, so that the predicted values of all observations are the same. There are several ways we can write this equation. First, the predicted values can be written as $E[Y_{t}] = \beta x$, where $x=1$. Assuming that the residuals are normally distributed, the model linking our predictions to observed data is written as 
$$y_t = \beta x + e_{t}, e_{t} \sim N(0,\sigma), x=1$$

An equivalent way to think about this model is that instead of the residuals as normally distributed with mean zero, we can think of the data $y_t$ as being drawn from a normal distribution with a mean of the intercept, and the same residual standard deviation:
$$Y_t \sim N(E[Y_{t}],\sigma)$$
Remember that in linear regression models, the residual error is interpreted as independent and identically distributed observation error.

To run this model using our package, we'll need to specify the response and predictor variables. The covariate matrix with an intercept only is a matrix of 1s. To double check, you could always look at 

```{r}
x = model.matrix(lm(Temp~1))
```

Fitting the model using our function is done with this code, 

```{r, warning=FALSE, message=FALSE, results='hide'}
lm_intercept = fit_stan(y = as.numeric(Temp), x = rep(1, length(Temp)),
  model_name = "regression")
```

Coarse summaries of *stanfit* objects can be examined by typing one of the following

```{r, eval=FALSE}
lm_intercept
#this is huge
#summary(lm_intercept)
```

But to get more detailed output for each parameter, you have to use the *extract()* function, 
```{r}
pars = extract(lm_intercept)
names(pars)
```

*extract()* will return the draws from the posterior for your parameters and any derived variables specified in your stan code.  In this case, our model is 
$$y_t = \beta \times 1 + e_t, e_t \sim N(0,\sigma)$$
so our estimated parameters are $\beta$ and $\sigma$.  Our stan code computed the derived variables: predicted $y_t$ which is $\hat{y}_t = \beta \times 1$ and the log-likelihood.  lp__ is the log posterior which is automatically returned.

**On your own, try to re-fit a regression model that includes the intercept and a slope, modeling the effect of Wind. What's the mean wind effect you estimate?**

We can then make basic plots or summaries of each of these parameters, 

```{r}
hist(pars$beta, 40, col="grey", xlab="Intercept", main="")
quantile(pars$beta, c(0.025,0.5,0.975))
```

One of the other useful things we can do is look at the predicted values of our model ($\hat{y}_t=\beta \times 1$) and overlay the data. The predicted values are *pars$pred*.

```{r}
plot(apply(pars$pred, 2, mean), main="Predicted values", lwd=2, 
  ylab="Wind", ylim= c(min(pars$pred), max(pars$pred)), type="l")
lines(apply(pars$pred, 2, quantile,0.025))
lines(apply(pars$pred, 2, quantile,0.975))
points(Wind, col="red")
```

### Burn-in and thinning

To illustrate the effects of the burn-in / warmup period, and thinning, we'll have you re-run the above model, but for just 1 MCMC chain (the default is 3).  

**Please make a plot of the time series of beta. Based on visual inspection, when does the chain converge? Second, calculate the ACF using acf() - would thinning more be appropriate?**

```{r eval = FALSE}
lm_intercept = fit_stan(y = Temp, x = rep(1, length(Temp)),
  model_name = "regression", 
  mcmc_list = list(n_mcmc = 1000, n_burn = 1, n_chain = 1, n_thin = 1))
```

## Linear regression with correlated errors

In our first model, the errors were independent in time. We're going to modify this to model autocorrelated errors. Autocorrelated errors are widely used in ecology and other fields -- for a greater discussion, see Morris and Doak (2002) Quantitative Conservation Biology. To make the errors autocorrelated, we start by defining the error in the first time step, ${e}_{1} = y_{1} - \beta$. The expectation of ${Y_t}$ in each time step is then written as 
$$E[{Y_t}] = \beta + \phi  e_{t-1}$$

In addition to affecting the expectation, the correlation parameter $\phi$ also affects the variance of the errors, so that 
$${ \sigma  }^{ 2 }={ \psi  }^{ 2 }\left( 1-{ \phi  }^{ 2 } \right)$$
Like in our first model, we assume that the data follows a normal likelihood (or equivalently that the residuals are normally distributed), $y_t = E[Y_t] + e_t$, or $Y_t \sim N(E[{Y_t}], \sigma)$. Thus, it is possible to express the subsequent deviations as ${e}_{t} = {y}_{t} - E[{Y_t}]$, or equivalently as ${e}_{t} = {y}_{t} - \beta -\phi  {e}_{t-1}$. 

We can fit this regression with autocorrelated errors by changing the model name to  'regression_cor' 

```{r eval = FALSE}
lm_intercept_cor = fit_stan(y = Temp, x = rep(1, length(Temp)),
  model_name = "regression_cor", 
  mcmc_list = list(n_mcmc = 1000, n_burn = 1, n_chain = 1, n_thin = 1))
```

## Random walk model 

All of the previous three models can be interpreted as observation error models. Switching gears, we can alternatively model error in the state of nature, creating process error models. A simple process error model that many of you may have seen before is the random walk model. In this model, the assumption is that the true state of nature (or latent states) are measured perfectly. Thus, all uncertainty is originating from process variation (for ecological problems, this is often interpreted as environmental variation). For this simple model, we'll assume that our process of interest (in this case, daily wind speed) exhibits no daily trend, but behaves as a random walk. 

$$y_t = y_{t-1} + e_{t}$$

And the ${e}_{t} \sim N(0, \sigma)$. Remember back to the autocorrelated model (or MA(1) models) that we assumed that the errors $e_t$ followed a random walk. In contrast, this model assumes that the errors are independent, but that the state of nature follows a random walk. Note also that this model as written doesn't include a drift term (this can be turned on / off using the *est_drift* argument).

**Please fit the random walk model to the temperature data. Our function can do this by using model_name = 'rw'. Once you've fitted the model, plot the predicted values and 95% CIs, as we did above in the regression model.** 

```{r, eval = FALSE}
rw = fit_stan(y = Temp, est_drift = FALSE, model_name = "rw")
```

## Autoregressive models 
A variation of the random walk model described previously is the autoregressive time series model of order 1, AR(1). This model is essentially the same as the random walk model but it introduces an estimated coefficient, which we'll call $\phi$. The parameter $\phi$ controls the degree to which the random walk reverts to the mean -- when $\phi$ = 1, the model is identical to the random walk, but at smaller values, the model will revert back to the mean (which in this case is zero). Also, $\phi$ can take on negative values, which we'll discuss more in future lectures. The math to describe the AR(1) model is: $$y_t = \phi y_{t-1} + e_{t}$$. 

**Our function can fit higher order AR models, but for now we just want you to fit an AR model and make a histogram of phi. **

```{r ar1, results=hide}
ar1 = fit_stan(y = Temp, x = matrix(1, nrow = length(Temp), ncol = 1), 
  model_name = "ar", est_drift=FALSE, P = 1)
```

**To see the effect of this increased flexibility in estimating the autocorrelation, make a plot of the predictions from the AR(1) model and the RW model**

## Univariate state-space models

At this point, we've fit models with observation or process error, but we haven't tried to estimate both simultaneously. We will do so here, and introduce some new notation to describe the process model and observation model. We use the notation ${x_t}$ to denote the latent state or state of nature (which is unobserved) at time $t$ and ${y_t}$ to denote the observed data. For introductory purposes, we'll make the process model autoregressive (similar to our AR(1) model),

$$x_{t} = \phi  x_{t-1} + e_{t}, e_{t} \sim N(0,q)$$

For the process model, there are a number of ways to parameterize the first 'state', and we'll talk about this more in the class, but for the sake of this model, we'll place a vague weakly informative prior on $x_1$, $x_1 \sim N(0, 0.01)$.Second, we need to construct an observation model linking the estimate unseen states of nature $x_t$ to the data $y_t$. For simplicitly, we'll assume that the observation errors are indepdendent and identically distributed, with no observation component. Mathematically, this model is 
$$Y_t \sim N(x_t, r)$$
In the two above models, we'll refer to $q$ as the standard deviation of the process variance and $r$ as the standard deviation of the observation error variance

**For this model, fit the above model with and without the autoregressive parameter $\phi$ and compare the estimated process and observation error variances. Code examples are given below**

```{r arrw, results=hide}
ss_ar = fit_stan(y = Temp, est_drift=FALSE, model_name = "ss_ar")
ss_rw = fit_stan(y = Temp, est_drift=FALSE, model_name = "ss_rw")
```

## Dynamic factor analysis

First load the plankton dataset from the MARSS package.
```{r dfa}
 library(MARSS)
 data(lakeWAplankton)
 # we want lakeWAplanktonTrans, which has been transformed
 # so the 0s are replaced with NAs and the data z-scored
 dat = lakeWAplanktonTrans
 # use only the 10 years from 1980-1989
 plankdat = dat[dat[,"Year"]>=1980 & dat[,"Year"]<1990,]
 # create vector of phytoplankton group names
 phytoplankton = c("Cryptomonas", "Diatoms", "Greens",
                   "Unicells", "Other.algae")
 # get only the phytoplankton
 dat.spp.1980 = t(plankdat[,phytoplankton])
 # z-score the data since we subsetted time
 dat.spp.1980 = dat.spp.1980-apply(dat.spp.1980,1,mean,na.rm=TRUE)
 dat.spp.1980 = dat.spp.1980/sqrt(apply(dat.spp.1980,1,var,na.rm=TRUE))
 #check our z-score
 apply(dat.spp.1980,1,mean,na.rm=TRUE)
 apply(dat.spp.1980,1,var,na.rm=TRUE)
```

Plot the data.
```{r plot.dfa}
#make into ts since easier to plot
dat.ts=ts(t(dat.spp.1980),frequency=12, start=c(1980,1))
par(mfrow=c(3,2),mar=c(2,2,2,2))
for(i in 1:5) 
  plot(dat.ts[,i], type="b",
       main=colnames(dat.ts)[i],col="blue",pch=16)
```

Run a 3 trend model on these data.
```{r dfa.3.trend, results=hide}
mod_3 = fit_dfa(y = dat.spp.1980, num_trends=3)
```

Rotate the estimated trends and look at what it produces.
```{r dfa.rot}
rot = rotate_trends(mod_3)
names(rot)
```

Plot the estimate of the trends.
```{r dfa.plot.trends}
matplot(t(rot$trends_mean),type="l",lwd=2,ylab="mean trend")
```

### Using leave one out cross-validation to select models

We will fit multiple DFA with different numbers of trends and use leave one out (LOO) cross-validation to choose the best model.

```{r dfa.5.models, results=hide}
mod_1 = fit_dfa(y = dat.spp.1980, num_trends=1)
mod_2 = fit_dfa(y = dat.spp.1980, num_trends=2)
mod_3 = fit_dfa(y = dat.spp.1980, num_trends=3)
mod_4 = fit_dfa(y = dat.spp.1980, num_trends=4)
mod_5 = fit_dfa(y = dat.spp.1980, num_trends=5)
```

We will compute the Leave One Out Information Criterion (LOOIC) using the loo package.  Like AIC, lower is better.

```{r looic}
library(loo)
loo(extract_log_lik(mod_1))$looic
```

Table of the LOOIC values:
  ```{r looic.table}
looics = c(
  loo(extract_log_lik(mod_1))$looic,
  loo(extract_log_lik(mod_2))$looic,
  loo(extract_log_lik(mod_3))$looic,
  loo(extract_log_lik(mod_4))$looic,
  loo(extract_log_lik(mod_5))$looic
  )
looic.table = data.frame(trends=1:5, LOOIC=looics)
looic.table
```

## Uncertainty intervals on states

We will look at the effect of missing data on the uncertainty intervals on estimates states using a DFA on the harbor seal dataset.

```{r harborseal.data}
data("harborSealWA")
#the first column is year
matplot(harborSealWA[,1],harborSealWA[,-1],type="l",
        ylab="Log abundance", xlab="")
```

Assume they are all observing a single trend.
```{r seal.fit, results=hide}
seal.mod= fit_dfa(y = t(harborSealWA[,-1]), num_trends = 1)
```

```{r seal.trend}
pars = extract(seal.mod)
```

```{r, echo = TRUE}
pred_mean = c(apply(pars$x, c(2,3), mean))
pred_lo = c(apply(pars$x, c(2,3), quantile, 0.025))
pred_hi = c(apply(pars$x, c(2,3), quantile, 0.975))

plot(pred_mean, type="l", lwd = 3, ylim = range(c(pred_mean, pred_lo, pred_hi)), main = "Trend")
lines(pred_lo)
lines(pred_hi)

```
